# scrap.test.task
Web scraping - Тестове завдання

## [Автоматизація дампа та експорту таблиці в Docker контейнері](./dockerized_cron/Dockerfile-scrap-postgres.md)

## Інструкція з розгортання (Linux)
1. Перейдіть до теки де плануєте встановити застосунок 
```
   $ cd some_dir/ 
```
2. Клонуйте з Git
```
   $ git clone https://github.com/semyon72/scrap.test.task.git
```
3. Перейдіть до утвореної теки з назвою `scrap.test.task` в середині `some_dir`
```
   $ cd scrap.test.task
```
4. Ініціалізуйте Python virtual environment
```
    ...some_dir/scrap.test.task$ python3 -m venv .venv 
```
5. Активізуйте virtual environment
```
    $ source .venv/bin/activate 
```

7. Всатновіть необхідні packages з requirements.txt
```
    (.venv) ... $ pip install -r requirements.txt
```

> Перед запуском застосунка необхідно мати встановлений та налогоджений PostgreSQL сервер та заповнений відповідним чином `.env` файл (опис нижче) 

8. Застосунок запускається за допомогою виконання `main.py`
```
    (.venv) ... $ python main.py
або
    /bin/bash --verbose -c 'cd /.../scrap.test.task/app; source ../.venv/bin/activate && python main.py'
або
    cd /.../scrap.test.task/app; source ../.venv/bin/activate && python main.py    
```  

## .env

### Назва файлу де буде зберігатись PID процесу, що виконується для запобігання подвійного запуску
- PIDFILE=.pid

### Налагодження бази даних
- DB_USER=scrap_test_task_user
- DB_PASSWORD=12345678
- DB_NAME=localhost/scrap_test_task

- DB_ECHO=True  # Виводить в кансоль всі SQL маніпуляції
- DB_DROP_TABLE=False  # True - Видаляє таблицю, перед тим як утворити нову, False - Викорисовується існуюча або утворюється нова якщо не існує.    

### Налагодження парсингу
- ROOT_URL=https://auto.ria.com/car/used/  # entrypoint  
- URL_PAGE_PARM_NAME=page  # query parameter що ідентифікує сторінку
- PAGE_START=12  # сторінка початку (включно)
> Можливо вказувати будьяке позитивне int але не більше ніж вказане в PAGE_END   
- PAGE_END=15  # сторінка кінця (включно)
> Можливо вказувати будьяке int значення одразу як сторінка з таким номером поверне 404 або результат парсингу не дасть жодного посилання на авто - робота буде завершеною
- SLEEP_AFTER_REQUEST=0  # час затримки між витребуванням сторінок
> Час затримки - Float значення в секундах і відповідає лише за одну ітерацію - інтервал між отриманням сторінки з посиланнями на данні щодо автомобілів.  
- DUMPS_DIR = './dumps'  # тека де зберігаються backup-s таблиці
> В теці вже присутній один архів для прикладу. Кожний запуск застосунка (на самому початку) генерує zip файл для поточного дня. У випадку декількох запусків протягом дня в zip додається окремий файл з часовою міткою. Дані з таблиці експортуються в csv фармат. 

### Налагодження перодичного виконання (Sheduler)
- SCHED_TIME_START  # Час старту 
> Формат - '22:00:34' або 22:00 ... . Якщо відсутнє (що еквівалентно None) або значення що не конвертується -> запуск негайно, інакше на вказаний момент. 
- SCHED_ONCE = True  # Одноразово чи ні 
> True або будьщо що не є \[False, 0, 0.0, '', None\] - буде виконано одноразово, інакше - періодично з інтервалом в день починаючи з SCHED_TIME_START.

## Примітки
> Також, для автоматичного запуску за розписом можливо використати будьякі зручні а можливо й вже наявні системи призначені для цього. Наприклад `cron` (Linux) чи `at` (Windows)
>
> Тека `work` містить лише робочі матеріали та може може бути видалена. 
 
